<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>hls4ml.optimization package &mdash; hls4ml 0.8.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_contributors.css" />

  
    <link rel="shortcut icon" href="../_static/hls4ml_logo.svg"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hls4ml.optimization.keras package" href="hls4ml.optimization.keras.html" />
    <link rel="prev" title="hls4ml.model.optimizer.passes package" href="hls4ml.model.optimizer.passes.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html">
            
              <img src="../_static/hls4ml_logo_navbar.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.9.0.dev59+gd63033b3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../status.html">Status and Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Setup and Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../details.html">Software Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../flows.html">Optimizer Passes and Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../command.html">Command Line Interface (deprecated)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference.html">Citation, Acknowledgments, and Contributors</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/hls-model.html">HLS Model Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/profiling.html">Profiling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/fifo_depth.html">FIFO Buffer Depth Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extension.html">Extension API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/accelerator.html">VivadoAccelerator Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_optimization.html">Hardware-aware Optimization API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Autogenerated API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hls4ml.backends.html">hls4ml.backends package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.converters.html">hls4ml.converters package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.model.html">hls4ml.model package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">hls4ml.optimization package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hls4ml.optimization.attributes">hls4ml.optimization.attributes module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hls4ml.optimization.config">hls4ml.optimization.config module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hls4ml.optimization.knapsack">hls4ml.optimization.knapsack module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hls4ml.optimization.scheduler">hls4ml.optimization.scheduler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hls4ml.optimization">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.report.html">hls4ml.report package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.utils.html">hls4ml.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.writer.html">hls4ml.writer package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">hls4ml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">hls4ml.optimization package</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fastmachinelearning/hls4ml/blob/main/docs/autodoc/hls4ml.optimization.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="hls4ml-optimization-package">
<h1>hls4ml.optimization package<a class="headerlink" href="#hls4ml-optimization-package" title="Permalink to this heading"></a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.optimization.keras.html">hls4ml.optimization.keras package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.builder">hls4ml.optimization.keras.builder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.config">hls4ml.optimization.keras.config module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.masking">hls4ml.optimization.keras.masking module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.reduction">hls4ml.optimization.keras.reduction module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.regularizers">hls4ml.optimization.keras.regularizers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras.utils">hls4ml.optimization.keras.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.keras.html#module-hls4ml.optimization.keras">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.optimization.objectives.html">hls4ml.optimization.objectives package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.objectives.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.objectives.html#module-hls4ml.optimization.objectives.gpu_objectives">hls4ml.optimization.objectives.gpu_objectives module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.objectives.html#module-hls4ml.optimization.objectives.vivado_objectives">hls4ml.optimization.objectives.vivado_objectives module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.objectives.html#module-hls4ml.optimization.objectives">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-hls4ml.optimization.attributes">
<span id="hls4ml-optimization-attributes-module"></span><h2>hls4ml.optimization.attributes module<a class="headerlink" href="#module-hls4ml.optimization.attributes" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.LayerAttributes">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.attributes.</span></span><span class="sig-name descname"><span class="pre">LayerAttributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inbound_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimization_attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.LayerAttributes" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class for storing layer information</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – Layer name</p></li>
<li><p><strong>layer_type</strong> (<em>keras.Layer</em>) – Layer type (e.g. Dense, Conv2D etc.)</p></li>
<li><p><strong>inbound_layers</strong> (<em>list</em>) – List of parent nodes, identified by name</p></li>
<li><p><strong>weight_shape</strong> (<em>tuple</em>) – Layer weight shape</p></li>
<li><p><strong>input_shape</strong> (<em>tuple</em>) – Layer input shape</p></li>
<li><p><strong>output_shape</strong> (<em>tuple</em>) – Layer output shape</p></li>
<li><p><strong>optimizable</strong> (<em>bool</em>) – Should optimizations (pruning, weight sharing) be applied to this layer</p></li>
<li><p><strong>optimization_attributes</strong> (<a class="reference internal" href="#hls4ml.optimization.attributes.OptimizationAttributes" title="hls4ml.optimization.attributes.OptimizationAttributes"><em>OptimizationAttributes</em></a>) – Type of optimization,
pruning or weight sharing, block shape and pattern offset</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – Additional information,
e.g. hls4mlAttributes; dictionary so it can be generic enough for different platforms</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.LayerAttributes.update_args">
<span class="sig-name descname"><span class="pre">update_args</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">updates</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.LayerAttributes.update_args" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.OptimizationAttributes">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.attributes.</span></span><span class="sig-name descname"><span class="pre">OptimizationAttributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">structure_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SUPPORTED_STRUCTURES.UNSTRUCTURED</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pruning</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_sharing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pattern_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">consecutive_patterns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.OptimizationAttributes" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class for storing layer optimization attributes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>structure_type</strong> (<em>enum</em>) – Targeted structure - unstructured, structured, pattern, block</p></li>
<li><p><strong>pruning</strong> (<em>boolean</em>) – Should pruning be applied to the layer</p></li>
<li><p><strong>weight_sharing</strong> (<em>boolean</em>) – Should weight sharing be applied to the layer</p></li>
<li><p><strong>block_shape</strong> (<em>tuple</em>) – Block shape if structure_type == block</p></li>
<li><p><strong>pattern_offset</strong> (<em>int</em>) – Length of each pattern if structure_type == pattern</p></li>
<li><p><strong>consecutive_patterns</strong> (<em>int</em>) – How many consecutive patterns are grouped together if structure_type == pattern</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>In the case of hls4ml, pattern_offset is equivalent to the number of weights processed in parallel</p></li>
<li><p>The pattern_offset is n_in * n_out / reuse_factor; default case (=1) is equivalent to no unrolling</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.get_attributes_from_keras_model">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.attributes.</span></span><span class="sig-name descname"><span class="pre">get_attributes_from_keras_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.get_attributes_from_keras_model" title="Permalink to this definition"></a></dt>
<dd><p>Given a Keras model, builds a dictionary of class attributes
Additional arguments (e.g. reuse factor), depend on the target hardware platform and are inserted later
Per-layer pruning sype (structured, pattern etc.), depend on the pruning objective and are inserted later</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.model</em>) – Model to extract attributes from</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Each key corresponds to a layer name, values are instances of LayerAttribute</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>model_attributes (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.get_attributes_from_keras_model_and_hls4ml_config">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.attributes.</span></span><span class="sig-name descname"><span class="pre">get_attributes_from_keras_model_and_hls4ml_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.get_attributes_from_keras_model_and_hls4ml_config" title="Permalink to this definition"></a></dt>
<dd><p>Given a Keras model and hls4ml configuration, builds a dictionary of class attributes
Per-layer pruning sype (structured, pruning etc.), depend on the pruning objective and are inserted later</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.model</em>) – Model to extract attributes from</p></li>
<li><p><strong>config</strong> (<em>dict</em>) – hls4ml dictionary</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Each key corresponds to a layer name, values are LayerAttribute instances</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>model_attributes (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.attributes.hls4mlAttributes">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.attributes.</span></span><span class="sig-name descname"><span class="pre">hls4mlAttributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">io_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_precision</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_precision</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallelization_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.attributes.hls4mlAttributes" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class for storing hls4ml information of a single layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> (<em>int</em>) – Number of inputs (rows) for Dense matrix multiplication</p></li>
<li><p><strong>n_out</strong> (<em>int</em>) – Number of outputs (cols) for Dense matrix multiplication</p></li>
<li><p><strong>io_type</strong> (<em>string</em>) – io_parallel or io_stream</p></li>
<li><p><strong>strategy</strong> (<em>string</em>) – Resource or Latency</p></li>
<li><p><strong>weight_precision</strong> (<a class="reference internal" href="hls4ml.model.html#hls4ml.model.types.FixedPrecisionType" title="hls4ml.model.types.FixedPrecisionType"><em>FixedPrecisionType</em></a>) – Layer weight precision</p></li>
<li><p><strong>output_precision</strong> (<a class="reference internal" href="hls4ml.model.html#hls4ml.model.types.FixedPrecisionType" title="hls4ml.model.types.FixedPrecisionType"><em>FixedPrecisionType</em></a>) – Layer output precision</p></li>
<li><p><strong>reuse_factor</strong> (<em>int</em>) – Layer reuse factor</p></li>
<li><p><strong>parallelization_factor</strong> (<em>int</em>) – Layer parallelization factor - [applicable to io_parallel Conv2D]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.config">
<span id="hls4ml-optimization-config-module"></span><h2>hls4ml.optimization.config module<a class="headerlink" href="#module-hls4ml.optimization.config" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.config.SUPPORTED_STRUCTURES">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.config.</span></span><span class="sig-name descname"><span class="pre">SUPPORTED_STRUCTURES</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.config.SUPPORTED_STRUCTURES" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="hls4ml.optimization.config.SUPPORTED_STRUCTURES.BLOCK">
<span class="sig-name descname"><span class="pre">BLOCK</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'block'</span></em><a class="headerlink" href="#hls4ml.optimization.config.SUPPORTED_STRUCTURES.BLOCK" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="hls4ml.optimization.config.SUPPORTED_STRUCTURES.PATTERN">
<span class="sig-name descname"><span class="pre">PATTERN</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'pattern'</span></em><a class="headerlink" href="#hls4ml.optimization.config.SUPPORTED_STRUCTURES.PATTERN" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="hls4ml.optimization.config.SUPPORTED_STRUCTURES.STRUCTURED">
<span class="sig-name descname"><span class="pre">STRUCTURED</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'structured'</span></em><a class="headerlink" href="#hls4ml.optimization.config.SUPPORTED_STRUCTURES.STRUCTURED" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="hls4ml.optimization.config.SUPPORTED_STRUCTURES.UNSTRUCTURED">
<span class="sig-name descname"><span class="pre">UNSTRUCTURED</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'unstructured'</span></em><a class="headerlink" href="#hls4ml.optimization.config.SUPPORTED_STRUCTURES.UNSTRUCTURED" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-hls4ml.optimization.knapsack">
<span id="hls4ml-optimization-knapsack-module"></span><h2>hls4ml.optimization.knapsack module<a class="headerlink" href="#module-hls4ml.optimization.knapsack" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.knapsack.solve_knapsack">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.knapsack.</span></span><span class="sig-name descname"><span class="pre">solve_knapsack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capacity</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CBC_MIP'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.knapsack.solve_knapsack" title="Permalink to this definition"></a></dt>
<dd><p>A function for solving the Knapsack problem</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> (<em>-</em>) – A one-dimensional array, where each entry is the value of an item</p></li>
<li><p><strong>weights</strong> (<em>-</em>) – An matrix, each row represents the weights of every item, in a given knapsack</p></li>
<li><p><strong>capacity</strong> (<em>-</em>) – A one-dimensional array, each entry is the maximum weights of a Knapsack</p></li>
<li><p><strong>implementation</strong> (<em>-</em>) – Algorithm to solve Knapsack problem - dynamic programming, greedy, branch and bound</p></li>
<li><p><strong>time_limit</strong> (<em>-</em>) – Limit (in seconds) after which the CBC or Branch &amp; Bound should
stop looking for a solution and return optimal so far</p></li>
<li><p><strong>scaling_factor</strong> (<em>-</em>) – Scaling factor for floating points values in CBC or B&amp;B</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>tuple containing</p>
<ul class="simple">
<li><p>optimal_value (float): The optimal values of elements in the knapsack</p></li>
<li><p>selected_items (list): A list of indices, corresponding to the selected elements</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><dl class="simple">
<dt>The general formulation of the Knapsack problem for N items and M knapsacks is:</dt><dd><p>max v.T &#64; x
s.t. A &#64; x &lt;= W
v ~ (N, 1) x ~ (N, 1) A ~ (M, N) W ~ (M, 1)
x_{i, j} = {0, 1} and &lt;= is the generalized, element-wise inequlaity for vectors</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Supported implementations:</dt><dd><ul>
<li><dl class="simple">
<dt>Dynamic programming:</dt><dd><ul>
<li><p>Optimal solution</p></li>
<li><p>Time complexity: O(nW)</p></li>
<li><p>Suitable for single-dimensional constraints and a medium number of items, with integer weights</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Branch and bound:</dt><dd><ul>
<li><p>Optimal</p></li>
<li><p>Solved using Google OR-Tools</p></li>
<li><p>Suitable for multi-dimensional constraints and a large number of items</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Branch and bound:</dt><dd><ul>
<li><p>Solution sub-optimal, but often better than greeedy</p></li>
<li><p>Solved using Google OR-Tools, with the CBC MIP Solver</p></li>
<li><p>Suitable for multi-dimensional constraints and a very high number of items</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Greedy:</dt><dd><ul>
<li><p>Solution sub-optimal</p></li>
<li><p>Time complexity: O(mn)</p></li>
<li><p>Suitable for highly dimensional constraints or a very high number of items</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Most implementations require integer values of weights and capacities;</dt><dd><p>For pruning &amp; weight sharing this is never a problem
In case non-integer weights and capacities are requires,
All of the values should be scaled by an appropriate scaling factor</p>
</dd>
</dl>
</li>
</ul>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.scheduler">
<span id="hls4ml-optimization-scheduler-module"></span><h2>hls4ml.optimization.scheduler module<a class="headerlink" href="#module-hls4ml.optimization.scheduler" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.BinaryScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.scheduler.</span></span><span class="sig-name descname"><span class="pre">BinaryScheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.BinaryScheduler" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#hls4ml.optimization.scheduler.OptimizationScheduler" title="hls4ml.optimization.scheduler.OptimizationScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">OptimizationScheduler</span></code></a></p>
<p>Sparsity updated by binary halving the search space; constantly updates lower and upper bounds
In the update step, sparsity is incremented, as the midpoint between previous sparsity and target sparsity (upper bound)
In the repair step, sparsity is decrement, as the midpoint between between the lower bound and previous sparsity</p>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.BinaryScheduler.repair_step">
<span class="sig-name descname"><span class="pre">repair_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.BinaryScheduler.repair_step" title="Permalink to this definition"></a></dt>
<dd><p>Method used when the neural architecture does not meet satisfy performance requirement for a given sparsity.
Then, the target sparsity is decreased according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55 [see ConstantScheduler for explanation]</p></li>
<li><p>BinaryScheduler, sparsity = 0.75, target = 1.0, previous = 0.5 -&gt; sparsity = (0.5 + 0.75) / 2 = 0.625</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.BinaryScheduler.update_step">
<span class="sig-name descname"><span class="pre">update_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.BinaryScheduler.update_step" title="Permalink to this definition"></a></dt>
<dd><p>Increments the current sparsity, according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55</p></li>
<li><p>BinaryScheduler, sparsity = 0.5, target = 1.0 -&gt; sparsity = 0.75</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.ConstantScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.scheduler.</span></span><span class="sig-name descname"><span class="pre">ConstantScheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.ConstantScheduler" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#hls4ml.optimization.scheduler.OptimizationScheduler" title="hls4ml.optimization.scheduler.OptimizationScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">OptimizationScheduler</span></code></a></p>
<dl class="simple">
<dt>Sparsity updated by a constant term, until</dt><dd><ol class="lowerroman simple">
<li><p>sparsity target reached OR</p></li>
<li><p>optimization algorithm stops requesting state updates</p></li>
</ol>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.ConstantScheduler.repair_step">
<span class="sig-name descname"><span class="pre">repair_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.ConstantScheduler.repair_step" title="Permalink to this definition"></a></dt>
<dd><p>Method used when the neural architecture does not meet satisfy performance requirement for a given sparsity.
Then, the target sparsity is decreased according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55 [see ConstantScheduler for explanation]</p></li>
<li><p>BinaryScheduler, sparsity = 0.75, target = 1.0, previous = 0.5 -&gt; sparsity = (0.5 + 0.75) / 2 = 0.625</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.ConstantScheduler.update_step">
<span class="sig-name descname"><span class="pre">update_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.ConstantScheduler.update_step" title="Permalink to this definition"></a></dt>
<dd><p>Increments the current sparsity, according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55</p></li>
<li><p>BinaryScheduler, sparsity = 0.5, target = 1.0 -&gt; sparsity = 0.75</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.OptimizationScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.scheduler.</span></span><span class="sig-name descname"><span class="pre">OptimizationScheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.OptimizationScheduler" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></p>
<p>Baseline class handling logic regarding target sparsity and its updates at every step</p>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.OptimizationScheduler.get_sparsity">
<span class="sig-name descname"><span class="pre">get_sparsity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.OptimizationScheduler.get_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.OptimizationScheduler.repair_step">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">repair_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.OptimizationScheduler.repair_step" title="Permalink to this definition"></a></dt>
<dd><p>Method used when the neural architecture does not meet satisfy performance requirement for a given sparsity.
Then, the target sparsity is decreased according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55 [see ConstantScheduler for explanation]</p></li>
<li><p>BinaryScheduler, sparsity = 0.75, target = 1.0, previous = 0.5 -&gt; sparsity = (0.5 + 0.75) / 2 = 0.625</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.OptimizationScheduler.update_step">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">update_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.OptimizationScheduler.update_step" title="Permalink to this definition"></a></dt>
<dd><p>Increments the current sparsity, according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55</p></li>
<li><p>BinaryScheduler, sparsity = 0.5, target = 1.0 -&gt; sparsity = 0.75</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.PolynomialScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.scheduler.</span></span><span class="sig-name descname"><span class="pre">PolynomialScheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">maximum_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.PolynomialScheduler" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#hls4ml.optimization.scheduler.OptimizationScheduler" title="hls4ml.optimization.scheduler.OptimizationScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">OptimizationScheduler</span></code></a></p>
<dl class="simple">
<dt>Sparsity updated by at a polynomial decay, until</dt><dd><ol class="lowerroman simple">
<li><p>sparsity target reached OR</p></li>
<li><p>optimization algorithm stops requesting state updates</p></li>
</ol>
</dd>
<dt>For more information, see Zhu &amp; Gupta (2016) -</dt><dd><p>‘To prune, or not to prune: exploring the efficacy of pruning for model compression’</p>
</dd>
</dl>
<p>Note, the implementation is slightly different, since TensorFlow Prune API depends on the total number of
epochs and update frequency.</p>
<p>In certain cases, a model might underperform at the current sparsity level, but perform better at a higher sparsity.
In this case, polynomial sparsity will simply jump to the next sparsity level
The model’s performance over several sparsity levels optimization is tracked and
toped after high loss over several trials (see top level pruning/optimization function)</p>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.PolynomialScheduler.repair_step">
<span class="sig-name descname"><span class="pre">repair_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.PolynomialScheduler.repair_step" title="Permalink to this definition"></a></dt>
<dd><p>Method used when the neural architecture does not meet satisfy performance requirement for a given sparsity.
Then, the target sparsity is decreased according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55 [see ConstantScheduler for explanation]</p></li>
<li><p>BinaryScheduler, sparsity = 0.75, target = 1.0, previous = 0.5 -&gt; sparsity = (0.5 + 0.75) / 2 = 0.625</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.scheduler.PolynomialScheduler.update_step">
<span class="sig-name descname"><span class="pre">update_step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.scheduler.PolynomialScheduler.update_step" title="Permalink to this definition"></a></dt>
<dd><p>Increments the current sparsity, according to the rule.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>ConstantScheduler, sparsity = 0.5, increment = 0.05 -&gt; sparsity = 0.55</p></li>
<li><p>BinaryScheduler, sparsity = 0.5, target = 1.0 -&gt; sparsity = 0.75</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>tuple containing</p>
<ul class="simple">
<li><p>updated (boolean) - Has the sparsity changed? If not, the optimization algorithm can stop</p></li>
<li><p>sparsity (float) - Updated sparsity</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-hls4ml.optimization">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-hls4ml.optimization" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.optimize_keras_model_for_hls4ml">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.</span></span><span class="sig-name descname"><span class="pre">optimize_keras_model_for_hls4ml</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">keras_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hls_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">increasing</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewinding_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_bad_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hls4ml-optimization'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knapsack_solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CBC_MIP'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regularization_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[1e-06,</span> <span class="pre">1.8478497974222906e-06,</span> <span class="pre">3.414548873833601e-06,</span> <span class="pre">6.30957344480193e-06,</span> <span class="pre">1.165914401179831e-05,</span> <span class="pre">2.1544346900318823e-05,</span> <span class="pre">3.9810717055349695e-05,</span> <span class="pre">7.356422544596421e-05,</span> <span class="pre">0.00013593563908785255,</span> <span class="pre">0.00025118864315095795,</span> <span class="pre">0.00046415888336127773,</span> <span class="pre">0.0008576958985908938,</span> <span class="pre">0.001584893192461114,</span> <span class="pre">0.0029286445646252374,</span> <span class="pre">0.0054116952654646375,</span> <span class="pre">0.01]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.optimize_keras_model_for_hls4ml" title="Permalink to this definition"></a></dt>
<dd><p>Top-level function for optimizing a Keras model, given hls4ml config and a hardware objective(s)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keras_model</strong> (<em>keras.Model</em>) – Model to be optimized</p></li>
<li><p><strong>hls_config</strong> (<em>dict</em>) – hls4ml configuration, obtained from hls4ml.utils.config.config_from_keras_model(…)</p></li>
<li><p><strong>objective</strong> (<a class="reference internal" href="hls4ml.optimization.objectives.html#hls4ml.optimization.objectives.ObjectiveEstimator" title="hls4ml.optimization.objectives.ObjectiveEstimator"><em>hls4ml.optimization.objectives.ObjectiveEstimator</em></a>) – </p></li>
<li><p><strong>Parameter</strong> – </p></li>
<li><p><strong>optimization</strong> (<em>hardware</em><em> or </em><em>user-defined objective of</em>) – </p></li>
<li><p><strong>scheduler</strong> (<em>Sparsity</em>) – </p></li>
<li><p><strong>scheduler</strong> – </p></li>
<li><p><strong>constant</strong> (<em>choose between</em>) – </p></li>
<li><p><strong>binary</strong> (<em>polynomial and</em>) – </p></li>
<li><p><strong>X_train</strong> (<em>np.array</em>) – Training inputs</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Training labels</p></li>
<li><p><strong>X_val</strong> (<em>np.array</em>) – Validation inputs</p></li>
<li><p><strong>y_val</strong> (<em>np.array</em>) – Validation labels</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size during training</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Maximum number of epochs to fine-tune model, in one iteration of pruning</p></li>
<li><p><strong>optimizer</strong> (<em>keras.optimizers.Optimizer</em><em> or </em><em>equivalent-string description</em>) – Optimizer used during training</p></li>
<li><p><strong>loss_fn</strong> (<em>keras.losses.Loss</em><em> or </em><em>equivalent loss description</em>) – Loss function used during training</p></li>
<li><p><strong>validation_metric</strong> (<em>keras.metrics.Metric</em><em> or </em><em>equivalent loss description</em>) – Validation metric, used as a baseline</p></li>
<li><p><strong>increasing</strong> (<em>boolean</em>) – If the metric improves with increased values;
e.g. accuracy -&gt; increasing = True, MSE -&gt; increasing = False</p></li>
<li><p><strong>rtol</strong> (<em>float</em>) – Relative tolerance;
pruning stops when pruned_validation_metric &lt; (or &gt;) rtol * baseline_validation_metric</p></li>
<li><p><strong>callbacks</strong> (<em>list of keras.callbacks.Callback</em>) – </p></li>
<li><p><strong>ranking_metric</strong> (<em>string</em>) – Metric used for ranking weights and structures;
currently supported l1, l2, saliency and Oracle</p></li>
<li><p><strong>local</strong> (<em>boolean</em>) – Layer-wise or global pruning</p></li>
<li><p><strong>verbose</strong> (<em>boolean</em>) – Display debug logs during model optimization</p></li>
<li><p><strong>rewinding_epochs</strong> (<em>int</em>) – Number of epochs to retrain model without weight freezing,
allows regrowth of previously pruned weights</p></li>
<li><p><strong>cutoff_bad_trials</strong> (<em>int</em>) – After how many bad trials (performance below threshold),
should model pruning / weight sharing stop</p></li>
<li><p><strong>directory</strong> (<em>string</em>) – Directory to store temporary results</p></li>
<li><p><strong>tuner</strong> (<em>str</em>) – Tuning algorithm, choose between Bayesian, Hyperband and None</p></li>
<li><p><strong>knapsack_solver</strong> (<em>str</em>) – Algorithm to solve Knapsack problem when optimizing;
default usually works well; for very large networks, greedy algorithm might be more suitable</p></li>
<li><p><strong>regularization_range</strong> (<em>list</em>) – List of suitable hyperparameters for weight decay</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Optimized model</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hls4ml.model.optimizer.passes.html" class="btn btn-neutral float-left" title="hls4ml.model.optimizer.passes package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hls4ml.optimization.keras.html" class="btn btn-neutral float-right" title="hls4ml.optimization.keras package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Machine Learning Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>